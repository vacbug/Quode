# Market Intelligence Data Collection & Analysis System

A comprehensive real-time market intelligence system that scrapes Twitter/X for Indian stock market discussions and converts textual data into quantitative trading signals.

## ğŸš€ Features

- **Real-time Data Collection**: Scrapes Twitter/X for Indian market hashtags (#nifty50, #sensex, #intraday, #banknifty)
- **Advanced Data Processing**: Text cleaning, normalization, and deduplication
- **Signal Generation**: Converts tweet content into quantitative trading signals using TF-IDF and sentiment analysis
- **Efficient Storage**: Parquet format with optimized schema
- **Memory-Efficient Visualization**: Streaming plots and data sampling techniques
- **Scalable Architecture**: Concurrent processing with rate limiting and anti-bot measures

## ğŸ“ Project Structure

```
market_intelligence/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection/
â”‚   â”‚   â”œâ”€â”€ twitter_scraper.py      # Twitter/X scraping engine
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py         # Rate limiting and anti-bot measures
â”‚   â”‚   â””â”€â”€ data_validator.py       # Data validation and cleaning
â”‚   â”œâ”€â”€ data_processing/
â”‚   â”‚   â”œâ”€â”€ text_processor.py       # Text cleaning and normalization
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py  # TF-IDF and feature extraction
â”‚   â”‚   â””â”€â”€ deduplication.py        # Data deduplication mechanisms
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ sentiment_analyzer.py   # Sentiment analysis engine
â”‚   â”‚   â”œâ”€â”€ signal_generator.py     # Trading signal generation
â”‚   â”‚   â””â”€â”€ signal_aggregator.py    # Composite signal creation
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ parquet_handler.py      # Parquet storage operations
â”‚   â”‚   â””â”€â”€ data_manager.py         # Data management utilities
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â”œâ”€â”€ streaming_plots.py      # Memory-efficient plotting
â”‚   â”‚   â””â”€â”€ dashboard.py            # Real-time dashboard
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py               # Configuration management
â”‚       â”œâ”€â”€ logger.py               # Logging utilities
â”‚       â””â”€â”€ helpers.py              # Helper functions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Raw scraped data
â”‚   â”œâ”€â”€ processed/                  # Processed data
â”‚   â””â”€â”€ signals/                    # Generated signals
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis_demo.ipynb         # Analysis demonstration
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_*.py                   # Unit tests
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ config.yaml                     # Configuration file
â””â”€â”€ main.py                         # Main execution script
```

## ğŸ› ï¸ Setup Instructions

### Prerequisites
- Python 3.8+
- Git

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd market_intelligence
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure the system**
   ```bash
   # Copy and edit configuration
   cp config.yaml.example config.yaml
   # Edit config.yaml with your settings
   ```

5. **Run the system**
   ```bash
   python main.py
   ```

##  Usage

### Data Collection
```python
from src.data_collection.twitter_scraper import TwitterScraper

scraper = TwitterScraper()
tweets = scraper.collect_tweets(target_hashtags=['#nifty50', '#sensex'])
```

### Signal Generation
```python
from src.analysis.signal_generator import SignalGenerator

generator = SignalGenerator()
signals = generator.generate_signals(tweets)
```

### Visualization
```python
from src.visualization.dashboard import Dashboard

dashboard = Dashboard()
dashboard.show_real_time_signals()
```

## ğŸ”§ Technical Approach

### Data Collection Strategy
- **Web Scraping**: Uses Selenium with rotating user agents and proxy support
- **Rate Limiting**: Implements exponential backoff and request queuing
- **Anti-Bot Measures**: Random delays, user agent rotation, and session management

### Data Processing Pipeline
1. **Text Cleaning**: Remove special characters, normalize Unicode, handle Indian languages
2. **Feature Engineering**: TF-IDF vectorization, sentiment scoring, hashtag extraction
3. **Deduplication**: Content-based hashing with fuzzy matching

### Signal Generation
- **Sentiment Analysis**: VADER sentiment scoring with domain-specific lexicon
- **Volume Analysis**: Engagement metrics normalization
- **Composite Signals**: Weighted combination of multiple features
- **Confidence Intervals**: Statistical significance testing

### Performance Optimizations
- **Concurrent Processing**: Async/await for I/O operations
- **Memory Management**: Streaming data processing and chunked operations
- **Storage Optimization**: Parquet compression and partitioning

## ğŸ“ˆ Sample Output

The system generates:
- **Raw Tweets**: 2000+ tweets with metadata
- **Processed Data**: Cleaned and normalized text data
- **Trading Signals**: Quantitative signals with confidence intervals
- **Visualizations**: Real-time charts and dashboards

## ğŸ§ª Testing

Run the test suite:
```bash
python -m pytest tests/
```

## ğŸ“ Configuration

Edit `config.yaml` to customize:
- Target hashtags and keywords
- Rate limiting parameters
- Storage settings
- Analysis parameters

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## âš ï¸ Disclaimer

This system is for educational and research purposes only. Trading decisions should not be based solely on social media sentiment analysis. Always consult with financial advisors before making investment decisions.
